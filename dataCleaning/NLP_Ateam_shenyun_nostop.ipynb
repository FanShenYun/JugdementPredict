{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg\n",
    "import jieba.analyse\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.pyplot import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載自己的判決檔案\n",
    "df = pd.read_csv('./onlyBillText.csv',header=None,sep=',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打開特徵詞庫\n",
    "feature = open('./feature/featureNew.txt', 'r', encoding = 'utf-8')\n",
    "lines = feature.readlines()\n",
    "feature_word = []\n",
    "for line in lines:\n",
    "    line = line.replace('\\n', '')\n",
    "    feature_word.append(line)\n",
    "feature_word=list(set(feature_word)) # 去重複的防呆\n",
    "    \n",
    "# 打開分詞庫\n",
    "all_dict = open('./dict/dict00.txt', 'r', encoding = 'utf-8')\n",
    "lines = all_dict.readlines()\n",
    "dict_word = []\n",
    "for line in lines:\n",
    "    line = line.replace('\\n', '')\n",
    "    dict_word.append(line)\n",
    "dict_word=list(set(dict_word)) # 去重複的防呆\n",
    "\n",
    "# 打開中立詞庫\n",
    "otherAll = open('./other/other00.txt', 'r', encoding = 'utf-8')\n",
    "lines = otherAll.readlines()\n",
    "other_word = []\n",
    "for line in lines:\n",
    "    line = line.replace('\\n', '')\n",
    "    other_word.append(line)\n",
    "other_word=list(set(other_word)) # 去重複的防呆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13172"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "事實及理由1、本件之犯罪事實及證據，除犯罪事實欄1第1行「陳章炫與陳俊瑋為父子關係，陳淑惠為陳章炫之媳婦」之記載，應補充更正為「陳章炫與陳俊瑋為父子關係，陳淑惠為陳章炫之媳婦，渠等間具有家庭暴力防治法第3條第3款所定之家庭成員關係」外，其餘均引用如附件檢察官聲請簡易判決處刑書之記載。2、按家庭暴力防治法所稱家庭暴力，係指家庭成員間實施身體或精神上不法侵害之行為。所謂家庭暴力罪，係指家庭成員間故意實施家庭暴力行為而成立其他法律所規定之犯罪，家庭暴力防治法第2條第1款、第2款分別定有明文。查被害人陳俊瑋係被告陳章炫之子，被害人陳淑惠為被告陳章炫之媳，業據被害人及被告陳明在卷，則被告與其2人間為直系血親及姻親關係，具有家庭暴力防治法第3條第3款所稱之家庭成員關係。被告陳章炫所為之公然侮辱罪，係屬於對家庭成員間實施精神上不法侵害之行為，自該當家庭暴力防治法所稱之家庭暴力罪。惟因家庭暴力防治法對於家庭暴力罪並無科處刑罰之規定，是以僅依刑法公然侮辱罪論罪科刑即可。故核被告陳章炫所為，係犯刑法第309條第1項之公然侮辱罪。被告以1行為侮辱被害人陳俊瑋、陳淑惠及員工黃正鑫3人，係1行為觸犯數罪名之想像競合犯，應依刑法第55條之規定，從1重仍論以刑法第309條第1項之公然侮辱罪。爰審酌被告未尊重他人之名譽法益，行為可訾，兼衡被告之素行、智識程度，犯罪之動機、目的、手段，及迄今未與被害人達成和解，以及被告犯罪後之態度等1切情狀，量處如主文所示之刑，並諭知易服勞役之折算標準，以示懲儆。3、依刑事訴訟法第449條第1項前段、第3項、第454條第2項，刑法第309條第1項、第55條、第42條第3項前段，刑法施行法第1條之1，逕以簡易判決處刑如主文。4、如不服本判決，得自收受送達之日起十日內向本院提出上訴狀，上訴於本院第2審合議庭（須附繕本）。中華民國101年4月17日刑事第23庭法官鄭水銓上列正本證明與原本無異。書記官莊依婷中華民國101年4月17日附錄本案論罪科刑法條全文：刑法第309條：公然侮辱人者，處拘役或300元以下罰金。以強暴犯前項之罪者，處1年以下有期徒刑、拘役或500元以下罰金。附件：臺灣板橋地方法院檢察署檢察官聲請簡易判決處刑書101年度偵字第4314號被告陳章炫男62歲（民國○○年○月○日生）住新北市○○區○○路2段147號4樓國民身分證統1編號：000000000號選任辯護人李漢鑫律師陳建偉律師上列被告因妨害名譽案件，業經偵查終結，認為宜以簡易判決處刑，茲將犯罪事實及證據並所犯法條分敘如下：犯罪事實1、陳章炫與陳俊瑋為父子關係，陳淑惠為陳章炫之媳婦，黃正鑫則為陳俊瑋所僱用之員工，陳章炫於民國100年4月12日下午3時45分許，在不特定多數人得共見共聞之新北市○○區○○街120號7樓辦公室會議室及個人工作室內，因與陳俊瑋就遷移公司地址需簽立股東同意書1事發生口角，竟基於公然侮辱之犯意，先以「你們壞事作盡，作惡多端」、「沒有人性、沒有理智」、「有病去看醫生」、「為虎作倀」等言語辱罵陳俊瑋及黃正鑫2人，並以「淑惠，我們家就是因為妳家破人亡」等語辱罵陳淑惠，足以貶抑陳俊瑋、黃正鑫及陳淑惠3人之人格評價。2、案經陳俊瑋、黃正鑫及陳淑惠訴由本署偵辦。證據並所犯法條1、訊據被告陳章炫矢口否認有何公然侮辱之犯行，辯稱：伊沒有辱罵告訴人陳俊瑋、黃正鑫及陳淑惠3人等語。經查，上揭事實，業據告訴人3人指訴綦詳，復有現場錄音影光碟片1片、錄音譯文及本署勘驗筆錄各1份附卷可稽，足見被告確有於上揭時地以上開言語辱罵告訴人3人，被告辯稱其未辱罵告訴人3人1節，顯係卸責之詞，不足採信，被告犯嫌堪以認定。2、核被告所為，係犯刑法第309條第1項之公然侮辱罪嫌。3、依刑事訴訟法第451條第1項聲請逕以簡易判決處刑。此致臺灣板橋地方法院中華民國101年90日15日檢察官魏子凱本件正本證明與原本無異中華民國101年90日29日書記官程蘧涵\n",
      "====================================================================================================\n",
      "4\n",
      "已有特徵詞:  309條第1項|矢口否認|卸責之詞|不足採信\n"
     ]
    }
   ],
   "source": [
    "i = 880\n",
    "words = df[1][i] # 2的部分就是i\n",
    "pay = df[2][i]\n",
    "print(pay)\n",
    "print(words)\n",
    "\n",
    "# 先看已有那些特徵詞\n",
    "jieba.load_userdict('./dict/dict00.txt')\n",
    "seg1 = jieba.lcut(words, cut_all = False)\n",
    "stop1 = open('./stopword/zh_stop00.txt', 'r', encoding = 'utf-8')\n",
    "lines = stop1.readlines()\n",
    "stop_word = []\n",
    "for line in lines:\n",
    "    line = line.replace('\\n', '')\n",
    "    stop_word.append(line)\n",
    "    \n",
    "rest_word = list(set(filter(lambda a : a not in stop_word, seg1)))\n",
    "feature = list(set(filter(lambda a : a in feature_word, rest_word)))\n",
    "print(\"=\"*100)\n",
    "print(len(feature))\n",
    "print('已有特徵詞: ', '|'.join(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在add_feature加入要新增的特徵詞\n",
    "add_feature = ['旅客不知情就欺騙']\n",
    "\n",
    "# 新增特徵詞庫\n",
    "for i in add_feature:\n",
    "    feature_word.append(i)\n",
    "    \n",
    "# 新增分詞庫\n",
    "for i in add_feature:\n",
    "    dict_word.append(i)\n",
    "    \n",
    "# 更新特徵詞庫文檔\n",
    "with open('./feature/featureNew.txt', 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in feature_word:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "temp_file.close()\n",
    "\n",
    "# 更新分詞庫文檔\n",
    "with open('./dict/dict00.txt', 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in dict_word:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "temp_file.close()\n",
    "\n",
    "# 重新讀取更新過的特徵詞庫\n",
    "feature = open('./feature/featureNew.txt', 'r', encoding = 'utf-8')\n",
    "lines = feature.readlines()\n",
    "feature_word = []\n",
    "for line in lines:\n",
    "    line = line.replace('\\n', '')\n",
    "    feature_word.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "分詞:  主任委員|齬|綽號|309條第1項|予新|鑑定人|辦公處|談判|防水層|滿於|犯刑|豐順|鑑定|你娘|阿滿|滲水|件|許止|齟|公然侮辱|加|不良|辦公|有口角|293|公然辱罵|嫌洵|基於公然侮辱之犯意|報告書|周佳誼|人屢經|阿中|強暴犯|蘇家弘|智識程度|內貌|開會|建築師|開開|孫玉玲|以足認|之妻|節本|幹|因陳|兩造|楊承翰|和解|1861|工作人員|起至|住處|張明隆|浴室|未與告訴人陳阿滿達成和解|代理|加人|名譽|共見共聞|之函文|鑑定會|人格|貶損|多數人|公會|樓層|否認犯行|主題|足以貶損|不特定多數人|簡易判決處刑|毀損|閱覽室|壁面|可資佐|該屋|開特|不特定人|勘等|318\n",
      "====================================================================================================\n",
      "3\n",
      "特徵詞:  309條第1項|幹|否認犯行\n",
      "====================================================================================================\n",
      "13\n",
      "剩餘斷詞:  公然辱罵,貶損,不特定人,多數人,足以貶損,不特定多數人,公然侮辱,有口角,共見共聞,簡易判決處刑,未與告訴人陳阿滿達成和解,基於公然侮辱之犯意,住處\n"
     ]
    }
   ],
   "source": [
    "# 讀取分詞庫\n",
    "\n",
    "jieba.load_userdict('./dict/dict00.txt')\n",
    "\n",
    "# 分詞\n",
    "\n",
    "seg1 = jieba.lcut(words, cut_all = False)\n",
    "\n",
    "# 開啟停用詞庫\n",
    "\n",
    "stop1 = open('./stopword/zh_stop00.txt', 'r', encoding = 'utf-8')\n",
    "lines = stop1.readlines()\n",
    "stop_word = []\n",
    "for line in lines:\n",
    "    line = line.replace('\\n', '')\n",
    "    stop_word.append(line)\n",
    "    \n",
    "rest_word = list(set(filter(lambda a : a not in stop_word, seg1)))\n",
    "feature = list(set(filter(lambda a : a in feature_word, rest_word)))\n",
    "otherTemp = list(set(filter(lambda a : a not in feature_word, rest_word)))\n",
    "other = list(set(filter(lambda a : a not in other_word, otherTemp)))\n",
    "\n",
    "print(len(rest_word))\n",
    "print('分詞: ', '|'.join(rest_word))\n",
    "print('='*100)\n",
    "print(len(feature))\n",
    "print('特徵詞: ', '|'.join(feature))\n",
    "print('='*100)\n",
    "print(len(other))\n",
    "print('剩餘斷詞: ', ','.join(other)) # 排除特徵詞跟已存中立詞，請從剩餘段詞內尋找要排除的中立詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在add_other加入要新增的中立詞\n",
    "# add_other = []\n",
    "\n",
    "# 覺得中立詞跟停止詞開始難分後就乾脆全丟中立詞\n",
    "add_other = other\n",
    "\n",
    "# 新增中立詞庫\n",
    "for i in add_other:\n",
    "    other_word.append(i)\n",
    "    \n",
    "# 更新中立詞庫文檔\n",
    "with open('./other/other00.txt', 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in other_word:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "temp_file.close()\n",
    "\n",
    "# 重新讀取更新過的中立詞庫\n",
    "otherAll = open('./other/other00.txt', 'r', encoding = 'utf-8')\n",
    "lines = otherAll.readlines()\n",
    "other_word = []\n",
    "for line in lines:\n",
    "    line = line.replace('\\n', '')\n",
    "    other_word.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 要新增的停用詞確認\n",
    "\n",
    "# jieba.load_userdict('./dict/dict.txt')\n",
    "# seg1 = jieba.lcut(words, cut_all = False)\n",
    "# stop1 = open('./stopword/zh_stop.txt', 'r', encoding = 'utf-8')\n",
    "# lines = stop1.readlines()\n",
    "# stop_word = []\n",
    "# for line in lines:\n",
    "#     line = line.replace('\\n', '')\n",
    "#     stop_word.append(line)\n",
    "    \n",
    "# rest_word = list(set(filter(lambda a : a not in stop_word, seg1)))\n",
    "# feature = list(set(filter(lambda a : a in feature_word, rest_word)))\n",
    "# otherTemp = list(set(filter(lambda a : a not in feature_word, rest_word)))\n",
    "# stop = list(set(filter(lambda a : a not in other_word, otherTemp)))\n",
    "\n",
    "# print(len(rest_word))\n",
    "# print('分詞: ', '|'.join(rest_word))\n",
    "# print('='*100)\n",
    "# print(len(feature))\n",
    "# print('特徵詞: ', '|'.join(feature))\n",
    "# print('='*100)\n",
    "# print(len(stop))\n",
    "# print('停用詞: ', '|'.join(stop)) # 前面剩餘用詞又排除了新增的中立詞，這邊剩下確定就是要丟停用詞的詞彙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 新增停用詞庫\n",
    "# for i in stop:\n",
    "#     stop_word.append(i)\n",
    "    \n",
    "# # 更新停用詞庫文檔\n",
    "# with open('./stopword/zh_stop.txt', 'w', encoding = 'utf-8') as temp_file:\n",
    "#     for item in stop_word:\n",
    "#         temp_file.write(\"%s\\n\" % item)\n",
    "# temp_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
